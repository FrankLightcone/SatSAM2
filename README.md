# SatSAM2

## Abstract

Satellite video tracking is a critical task in remote sensing, enabling large-scale, continuous monitoring for applications such as intelligent transportation, urban surveillance, and defense. However, it poses significant challenges due to low spatial resolution, large scene coverage, frequent occlusions, and the presence of numerous visually similar small objects. While recent foundation models like the Segment Anything Model (SAM) and its video extension SAM2 offer strong zero-shot segmentation capabilities, their direct application to satellite videos suffers from domain gaps, occlusion-induced drift, and lack of structural motion priors.
We propose SatSAM2, a novel promptable satellite video tracker that integrates SAM2 with a motion-constrained finite-state machine and a Kalman Filter-based Constrained Motion Model (KFCMM). SatSAM2 leverages the rigidity and consistent motion of satellite targets to model object dynamics more effectively. It combines visual evidence from SAM2 with historical motion cues to mitigate failures caused by occlusion and background clutter. A multi-score memory selection strategy further enhances tracking reliability by jointly considering segmentation confidence, object presence, and motion consistency.
To support large-scale evaluation, we introduce MatrixCity-Sat, a synthetic satellite video dataset with over 1,500 sequences and 157,900 annotated frames under varying viewpoints, lighting, and occlusion scenarios. Extensive experiments demonstrate that SatSAM2 consistently outperforms conventional trackers, SAM2, and its enhanced variants, particularly under challenging remote sensing conditions. Our findings highlight the importance of integrating motion priors into promptable foundation models for robust satellite video tracking. 
